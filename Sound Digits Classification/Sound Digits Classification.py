# -*- coding: utf-8 -*-
"""A4_p1_[hi3334].ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XcXbcG78PjOZOkxjkC2ZAUsAOdKVbpIE
"""

# import tensorflow
! pip install pydub
import tensorflow as tf
from tensorflow.keras import *
import os
import pathlib
import matplotlib as plt
import numpy as np
from tensorflow.keras import layers
from tensorflow.keras import models
from IPython import display
# import tensorflow datasets
import tensorflow_datasets as TFDatasets

"""
-----------------------------------------------------------------------------------------------
Step 1: Load dataset spoken_digit [30 points]
Load dataset from Tensorflow Datasets;
Set batch size to 64;
Preprocess the dataset to get spectogram by applying the short-time Fourier transform; 
Split the dataset with 70% of training, 10% of validation, and 20% of testing;
-----------------------------------------------------------------------------------------------
"""

# load mnist dataset
batch_size = 64
dataset = list(TFDatasets.load('spoken_digit',
                              split = 'train',
                              shuffle_files=True))
input_length = 24000                                                     
# initialize zero padding function and fourier transform
def stft(waveform: tf.Tensor, label: int) -> tuple[tf.Tensor, int]:
    waveform = tf.cast(waveform, dtype=tf.float32)
    spectrogram = tf.signal.stft( waveform, frame_length=255, frame_step=128)
    spectrogram = tf.abs(spectrogram)
    spectrogram = spectrogram[..., tf.newaxis]
    label = []
    for i in range(len(dataset)):
        label.append(dataset[i]['label'].numpy())
    label = tf.convert_to_tensor(label)
    label = tf.one_hot(label, 10, on_value=1, off_value=0) 
    #label = TFDatasets.features.ClassLabel(num_classes=10),
    return spectrogram, label
# zero_padding    
def zero_padding(waveform, array_size):
    padded_dataset =[]
    for i in range(len(waveform)):
        padding_size = array_size - len(waveform[i]['audio'].numpy())
        padded_dataset.append(np.pad(waveform[i]['audio'].numpy(), (0, padding_size), 'constant', constant_values=(0)))
    return padded_dataset 

padded_dataset = zero_padding(dataset, input_length)
spectrogram = stft(padded_dataset)

# mapping to dataset
dataset = list(tf.data.Dataset.from_tensor_slices((spectrogram, label)))

# split dataset
# we have only 2500 training datasets for spoken_digit data
# let split it into test, train, and validation 


training_dataset = TFDatasets.Split('train[:70%]')
training_label = TFDatasets.Split('train[:70%]')
validation_dataset = TFDatasets.Split('validation[70%:80%]')
validation_label = TFDatasets.Split('validation[70%:80%]')
testing_dataset = TFDatasets.Split('test[80%:]')
testing_label = TFDatasets.Split('test[80%:]')

# split dataset
# we have only 2500 training datasets for spoken_digit data
# let split it with spectogram and labels for training process
x_train = training_dataset
x_train = spectrogram[:1750]

y_train = training_label
y_train = label[:1750]

x_valid = validation_dataset
x_valid = spectrogram[1750 : 2000]

y_valid = validation_label
y_valid = label[1750 : 2000]

x_test = testing_dataset
x_test = spectrogram[2000 : 2500]

y_test = testing_label
y_test = label[2000 : 2500]

"""
Step 2: Define CRNN model [30 points]
1. Resize layer to resize the spectrogram to 32x32
"""
model = Sequential()
model.add(layers.Resizing(32,32))
#2. Normalization layer to normalize the input data based on its mean and standard deviation
model.add(layers.Normalization(axis=None))
#3. Conv2D layer named 'conv1' with 64 filters, 3x3 kernel size, same padding, and relu activation
model.add(layers.Conv2D(64, (3,3), strides=(1,1), padding='same', activation='relu', name='conv1'))
#4. BatchNormalization layer to normalize axis 3
model.add(layers.BatchNormalization(axis=3))
#5. MaxPooling2D layer to pool the features
model.add(layers.MaxPool2D(pool_size=(2,2), strides=(1,1), padding='valid'))
#6. Conv2D layer named 'conv2' with 64 filters, 3x3 kernel size, same padding, and relu activation
model.add(layers.Conv2D(64, (3,3), strides=(1,1), padding='same', activation='relu', name='conv2'))
#7. BatchNormalization layer to normalize axis 3
model.add(layers.BatchNormalization(axis=3))
#8. MaxPooling2D layer to pool the features
model.add(layers.MaxPool2D(pool_size=(2,2), strides=(1,1), padding='valid'))
#9. Permute layer to permute the frequency axis and time axis
model.add(layers.Permute((2,1,3), name='permute'))
#10. Reshape the permuted output to (-1, shape[1], shape[2] * shape[3])
model.add(layers.Reshape((-1, 30*30*64)))
#11. A GRU layer named 'gru1' with 512 units
model.add(layers.GRU(512, activation='tanh', recurrent_activation='sigmoid', name='gru1'))
#12. A GRU layer named 'gru2' with 512 units
model.add(layers.Reshape((-1, 512)))
model.add(layers.GRU(512, activation='tanh', recurrent_activation='sigmoid', name='gru2'))
#13. A Dropout layer with dropout ratio 0.5
model.add(layers.Dropout(0.5))
#14. A Dense layer to do the classification
model.add(layers.Dense(10, activation='softmax'))

model.build(x_train.shape)
model.summary()

"""
-----------------------------------------------------------------------------------------------
Step 3: Compile the model [10 points]
Adam optimizer with 0.001 learning rate, multiplies 0.9 for each epoch;
Categorical crossentropy as loss function;
Accuracy as the metrics;
-----------------------------------------------------------------------------------------------
"""
model.compile(loss = tf.keras.losses.CategoricalCrossentropy(name = 'loss'),
              optimizer = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9),
              metrics = [metrics.categorical_accuracy])

"""
-----------------------------------------------------------------------------------------------
Step 4: Train the model with training dataset [10 points]
Set epoch size to 30;
TensorBoard Callback to record the metrics for each epoch;
Checkpoint Callback checkpoints;
-----------------------------------------------------------------------------------------------
"""
logdir = "logs/scalars"
tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)

"""
log_dir="logs/train_data/" + datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir= 'log_dir', histogram_freq=1)
train_summary_writer = tf.summary.create_file_writer(log_dir)
ckpt_path = "ckpt/checkpoint"
"""

EPOCHS = 30
history = model.fit(
    x_train,
    y_train,
    validation_data=(x_valid, y_valid),
    epochs=EPOCHS,
    batch_size = 64,
    callbacks=[tf.keras.callbacks.TensorBoard(log_dir=logdir)]
)

# Commented out IPython magic to ensure Python compatibility.
# Tensorboard
# %reload_ext tensorboard
# %tensorboard --logdir logs/scalars

"""
-----------------------------------------------------------------------------------------------
Step 5: Evaluate the model with testing dataset [10 points]
-----------------------------------------------------------------------------------------------
"""

scores = model.evaluate(x_test, y_test, verbose=2)
print("loss:", scores[0])
print("accuracy:", scores[1])

"""
-----------------------------------------------------------------------------------------------
Step 6: Remember to submit results to Canvas. [10 points]
A screenshot of TensorBoard;
A screenshot of the training and testing procedure;
-----------------------------------------------------------------------------------------------
"""
"""
loss: 0.42530372738838196
accuracy: 0.878000020980835
"""