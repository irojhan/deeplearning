# -*- coding: utf-8 -*-
"""A2_P1_[hi3334].ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1K8FFcLY8-ibUtUZ6u18lVsQul2ljCGf3
"""

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import *
from tensorflow.keras import layers
from tensorflow.keras.metrics import Metric
import numpy as np
import time
import datetime
import os
import matplotlib.pyplot as plt

"""
-----------------------------------------------------------------------------------------------
Step 1: Load dataset MNIST [10 points]
Load dataset from Keras;
Wrap to tensorflow dataset;
Set epoch size to 20;
Set batch size to 256 (or fit your GPU GRAM size);
-----------------------------------------------------------------------------------------------
"""
mnist = tf.keras.datasets.mnist
num_classes = 10  # 0 to 9 digits
num_features = 784  # 28*28
batch_size = 256
epochs = 20
(x_train, y_train),(x_test, y_test) = mnist.load_data()


# data normalization
x_train=x_train.reshape(x_train.shape[0],784).astype("float32")
x_test=x_test.reshape(x_test.shape[0],784).astype("float32")
x_train = x_train / 255.0
x_test = x_test / 255.0


training_dataset = training_dataset.shuffle(60000).batch(256)
testing_dataset = testing_dataset.batch(256)

# one-hot encode target set
#y_trains = tf.one_hot(y_true, num_classes)

y_trains = np.zeros((num_classes,len(y_train)), dtype = 'float32')
for count,target in enumerate(y_train):
    y_trains[target,count] = 1

y_tests = np.zeros((num_classes,len(y_test)), dtype = 'float32')
for count,target in enumerate(y_test):
    y_tests[target,count] = 1    
# prediction values
#y_pred = tf.clip_by_value(y_pred, 1/100, 1)
#correct_prediction = tf.equal(tf.argmax(y_pred, 1), tf.cast(y_true, tf.int64))

"""
-----------------------------------------------------------------------------------------------
Step 2: Define logistic regression model [15 points]
Initialize the model instance;
Use CategoricalCrossentropy as your loss function;
Use CategoricalAccuracy as your metrics;
Use Adam with its default settings as your optimizer;
-----------------------------------------------------------------------------------------------
"""

class LogisticRegression(tf.Module):
    W: tf.Variable
    b: tf.Variable
    #x = tf.compat.v1.placeholder
    #y = tf.placeholder(dtype=tf.float32, shape=(None, 10))
    
    #\\TODO: define the __init__ method and complete the forward pass (__call__)


    def __init__(self):
        super(LogisticRegression, self).__init__()
        self.W = tf.Variable(tf.ones([784, 10]), name="weight")
        self.b = tf.Variable(tf.zeros([10]), name="bias")
        #self.x = tf.compat.v1.placeholder(dtype=tf.float32, shape=(None, 784))

    def __call__(self, input_data: tf.Tensor) -> tf.Tensor:
        input_ = tf.add(tf.matmul(input_data, self.W), self.b, name="input_2_output")
        y: tf.Tensor = tf.nn.softmax(input_) 
        return y


 
model = LogisticRegression()
loss_fn =tf.keras.losses.CategoricalCrossentropy()
acc_fn = tf.keras.metrics.CategoricalAccuracy()
optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)

"""
-----------------------------------------------------------------------------------------------
Step 3: train the model [20 points]
Use summrary writer record TensorBoard data;
Use Checkpoint to save checkpoints;
Use training_dataset to train the model;
-----------------------------------------------------------------------------------------------
"""
train_log_dir = 'logs/gradient_tape'
train_summary_writer = tf.summary.create_file_writer(train_log_dir)
ckpt_path = "ckpt/checkpoint"

t_loss = tf.keras.metrics.Mean('train_loss', dtype=tf.float32)

# use enumerate to iterate over data  
train_iter = iter(training_dataset)
for i in range(epochs):
    # do some setup
    for step in range(batch_size):
        (xb_train, yb_train) = next(train_iter)
for epoch in range(epochs):
    
    for iteration,(xb_train, yb_train) in enumerate(training_dataset):
        with tf.GradientTape() as tape:
            y =model(xb_train,training=False)
            yb_train = tf.one_hot(yb_train, depth=10)
            loss: tf.Tensor = loss_fn(yb_train,y)
            acc: tf.Tensor = acc_fn(yb_train,y) 
            
        grads: list[tf.Tensor] = tape.gradient(loss,[model.W,model.b])
        optimizer.apply_gradients(zip(grads,[model.W,model.b]))
        t_loss(loss)
        
        if  iteration % 50==0:  # the number of iterations meets a requirement
            print("epoch %d: Iteration %d: loss=%f, accuracy=%f" % (epoch, iteration, loss, acc)) # iteration
            with train_summary_writer.as_default():
                tf.summary.scalar('loss',t_loss.result(),step=iteration)
                tf.summary.scalar('acc',acc_fn.result(),step=iteration)
        
        if iteration %2000==0:# the number of iterations meets a requirement
            checkpoint =train_summary_writer
            checkpoint.write(ckpt_path)

# Commented out IPython magic to ensure Python compatibility.
# %reload_ext tensorboard
# %tensorboard --logdir logs/gradient_tape

"""
-----------------------------------------------------------------------------------------------
Step 4: Test the model [15 points]
Record each loss and accuracy in the list;
Calculate the mean loss and accuracy;
Hint: If the accuracy is less then 85%, increase your epoch size and use a different random function for the weights.
Final accuracy = 0.927958
-----------------------------------------------------------------------------------------------
"""


loss_list: list[tf.Tensor] = list()
accuracy_list: list[tf.Tensor] = list()
x_test = np.array(x_test)
print("x_test", x_test.shape)
print("x_test type")
print(x_test.shape)
print(y_test.shape)
y_res = model.__call__(x_test)
l = loss_fn(y_test, y_res)
print("Loss", l)
a = acc_fn(y_test, y_res)
print("acc", a)
print("Testing Result: loss=%f, accuracy=%f" % (l, a))  # iteration

         
for index, yResultArray in y_res:
  testResultArray.insert(yResultArray[y_test[index]] == 1)
    y = model.__call__(x_test)
    l = loss_fn(y_test, y)
    a = acc_fn(y_test, y)

loss_eval = tf.reduce_mean(loss_list)
acc_eval = tf.reduce_mean(accuracy_list)
tf.print("eval_loss=%f, eval_acc=%f" % (loss_eval, acc_eval))


-----------------------------------------------------------------------------------------------
#Remember to submit results to Canvas. [10 points]
-----------------------------------------------------------------------------------------------