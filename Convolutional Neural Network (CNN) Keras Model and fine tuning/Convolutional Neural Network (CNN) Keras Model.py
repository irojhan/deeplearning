# -*- coding: utf-8 -*-
"""A3_P1_[hi3334].ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kUNq1Ke6-5Q4GG9OEG7VSxYCbJJ4_1VT
"""

#first of all we must install  -q tfds-nightly tensorflow matplotlib
!pip install -q tfds-nightly tensorflow matplotlib
import matplotlib.pyplot as plt
import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from keras import losses
from keras import metrics
from keras import optimizers
from sklearn.preprocessing import LabelBinarizer
import tensorflow_datasets as tfds
# Part 1 [5 points] Load the MNIST dataset by using TensorFlow datasets.
(training_data, training_labels), (testing_data, testing_labels) = tfds.as_numpy(tfds.load('mnist', split = ['train', 'test'],batch_size=-1,as_supervised=True))

# Part 2 [35 points] Use functional API in TensorFlow to create a Keras model named CNN.
CNN = keras.Input(shape=(28, 28, 1))
conv1 = layers.Conv2D(32, activation="relu", padding="same", strides=(1,1), kernel_size=(3,3))
x=conv1(CNN)
maxpool2=layers.MaxPool2D(pool_size=(2,2),strides=(1,1),padding="valid")(x)
conv3 = layers.Conv2D(64, activation="relu", padding="same", strides=(1,1), kernel_size=(3,3))(maxpool2)
maxpool4=layers.MaxPool2D(pool_size=(2,2),strides=(1,1),padding="valid")(conv3)
conv5 = layers.Conv2D(128, activation="relu", padding="same", strides=(1,1), kernel_size=(3,3))(maxpool4)
maxpool6=layers.MaxPool2D(pool_size=(2,2),strides=(1,1),padding="valid")(conv5)
flatten=layers.Flatten()(maxpool6)
dense1=layers.Dense(1024, activation="relu")(flatten)
dense2=layers.Dense(256, activation="relu")(dense1)
classification=layers.Dense(10, activation="Softmax")(dense2)
# creating model 
model = keras.Model(inputs=CNN, outputs=classification, name="mnist_model")
label_as_binary = LabelBinarizer()
training_y_labels = label_as_binary.fit_transform(training_labels)
#Part 3 [10 points] Use Adam optimizer with a 0.0001 learning rate, categorical cross entropy loss, and categorical accuracy to compile the model.

model.compile(loss = 'categorical_crossentropy',
              optimizer = keras.optimizers.Adam(learning_rate=0.0001),
              metrics = [metrics.categorical_accuracy])

#Part 4 [10 points] Train your model for 10 epochs with a batch size of 128. Make a screenshot of your training procedure to A3_P1_[your AccessID].jpg
history = model.fit(training_data, training_y_labels, batch_size=128, epochs=10)

#Part 5 [10 points] Use your testing dataset to evaluate your trained model then save your model as A3_P1_[your AccessID].model.
label_as_binary = LabelBinarizer()
training_y_labels = label_as_binary.fit_transform(testing_labels)

scores_testing = model.evaluate(testing_data, training_y_labels, verbose=2)
print("Test score loss:", scores_testing[0])
print("Test score accuracy:", scores_testing[1])

!mkdir -p saved_model
model.save('A3_P1_[hi3334].model')
"""
Test score loss: 0.05819021537899971
Test score accuracy: 0.9872000217437744

"""

!mkdir -p saved_model
model.save('A3_P1_[hi3334]')
! ls saved_model

! ls saved_model/A3_P1_[hi3334]